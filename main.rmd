---
title: "R Evaluacion"
author: "Rafael Camarero Rodríguez"
date: "2025-03-02"
output: pdf_document
---
# Sección 1: Código fuente y Resultados de la ejecución
La presente sección se ha generado usando Rmarkdown, en dataspell, el código fuente se puede encontrar tanto en formato r como rmd adjuntado en la entrega y en github https://github.com/Rcamrods/R_evaluacion/
```{r}
install.packages("caret", repos = "https://cran.rstudio.com/")
library(caret)
install.packages("AUC", repos = "https://cran.rstudio.com/")
library(AUC)
install.packages("ROCR", repos = "https://cran.rstudio.com/")
library(ROCR)
install.packages("tinytex", repos = "https://cran.rstudio.com/")
library(tinytex)
```
```{r}
print("1. Cargar datos en R")
print("====================")

set.seed(12345)

data <- read.csv("tic-tac-toe.data.txt", header = FALSE)

names(data) <- c("top_left", "top_mid", "top_right",
                 "mid_left", "mid_mid", "mid_right",
                 "bot_left", "bot_mid", "bot_right",
                 "Class")

missing_vals <- sum(is.na(data))
cat("Numero de valores faltantes:", missing_vals, "\n")
```
```{r}
print("2. Partir datos en train y test")
print("===============================")
trainIndex <- createDataPartition(data$Class, p = 0.7, list = FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

cat("Proporcion en entrenamiento:\n")
print(prop.table(table(trainData$Class)))
cat("Proporcion en test:\n")
print(prop.table(table(testData$Class)))

```
```{r}
print("3. Entrenamiento usando validacion cruzada")
print("==========================================")
control <- trainControl(method = "repeatedcv", number = 10, repeats = 1, classProbs = TRUE)

model_nb   <- train(Class ~ ., data = trainData, method = "nb",        trControl = control)
model_dt   <- train(Class ~ ., data = trainData, method = "rpart",     trControl = control)
model_nn   <- train(Class ~ ., data = trainData, method = "nnet",      trControl = control, trace = FALSE)
model_knn  <- train(Class ~ ., data = trainData, method = "knn",       trControl = control)
model_svm  <- train(Class ~ ., data = trainData, method = "svmLinear", trControl = control)

results <- resamples(list(NB = model_nb, DecisionTree = model_dt,
                          NeuralNet = model_nn, KNN = model_knn,
                          SVM = model_svm))
summary(results)

pred_nb  <- predict(model_nb, newdata = testData)
pred_dt  <- predict(model_dt, newdata = testData)
pred_nn  <- predict(model_nn, newdata = testData)
pred_knn <- predict(model_knn, newdata = testData)
pred_svm <- predict(model_svm, newdata = testData)


testData$Class <- factor(testData$Class, levels = c("negative", "positive"))

pred_nb <- factor(pred_nb, levels = levels(testData$Class))
pred_dt <- factor(pred_dt, levels = levels(testData$Class))
pred_nn <- factor(pred_nn, levels = levels(testData$Class))
pred_knn <- factor(pred_knn, levels = levels(testData$Class))
pred_svm <- factor(pred_svm, levels = levels(testData$Class))

eval_nb <- postResample(pred_nb, testData$Class)
eval_dt  <- postResample(pred_dt,  testData$Class)
eval_nn  <- postResample(pred_nn,  testData$Class)
eval_knn <- postResample(pred_knn, testData$Class)
eval_svm <- postResample(pred_svm, testData$Class)

cat("Naive Bayes:\n");  print(eval_nb)
cat("Decision Tree:\n"); print(eval_dt)
cat("Neural Network:\n"); print(eval_nn)
cat("KNN:\n");          print(eval_knn)
cat("SVM:\n");  print(eval_svm)
```
```{r}
print("4. Mostrar matrices de confusion y aniadir el AUC a la tabla de accuracy y kappa")
print("================================================================================")

cm_nb <- confusionMatrix(pred_nb, testData$Class)
cm_dt <- confusionMatrix(pred_dt, testData$Class)
cm_nn <- confusionMatrix(pred_nn, testData$Class)
cm_knn <- confusionMatrix(pred_knn, testData$Class)
cm_svm <- confusionMatrix(pred_svm, testData$Class)

print("Matrices de confusion")
cat("Naive Bayes:\n");  print(cm_nb$table)
cat("Decision Tree:\n"); print(cm_dt$table)
cat("Neural Network:\n"); print(cm_nn$table)
cat("KNN:\n");          print(cm_knn$table)
cat("SVM:\n");  print(cm_svm$table)

calcular_auc <- function(model, testData) {
  prob <- predict(model, newdata = testData, type = "prob")
  roc_obj <- roc(prob$positive, testData$Class)
  auc_val <- auc(roc_obj)
  return(auc_val)
}

auc_nb  <- calcular_auc(model_nb,  testData)
auc_dt  <- calcular_auc(model_dt,  testData)
auc_nn  <- calcular_auc(model_nn,  testData)
auc_knn <- calcular_auc(model_knn, testData)
auc_svm <- calcular_auc(model_svm, testData)

print("Tabla con accuracy, kappa y auc")
resultados_test <- data.frame(
  Modelo = c("Naive Bayes", "Decision Tree", "Neural Network", "Nearest Neighbour", "SVM (linear)"),
  Accuracy = c(eval_nb[1], eval_dt[1], eval_nn[1], eval_knn[1], eval_svm[1]),
  Kappa = c(eval_nb[2], eval_dt[2], eval_nn[2], eval_knn[2], eval_svm[2]),
  AUC = c(auc_nb, auc_dt, auc_nn, auc_knn, auc_svm)
)
print(resultados_test)
```
```{r}
print("5. Representar curvas ROC")
print("=========================")
trazar_roc <- function(model, testData, color) {
  # 5. a) recalculamos las predicciones con el parámetro prob
  prob <- predict(model, newdata = testData, type = "prob")
  # 5. b) creamos el objeto predicción
  pred <- prediction(prob$positive, testData$Class)
  # 5. c) calculamos TPR y FPR
  perf <- performance(pred, "tpr", "fpr")
  # 5. d) Dibujamos la curva
  plot(perf, col = color, lwd = 2, add = TRUE)
}

prob_nb <- predict(model_nb, newdata = testData, type = "prob")
pred_nb <- prediction(prob_nb$positive, testData$Class)
perf_nb <- performance(pred_nb, "tpr", "fpr")
plot(perf_nb, col = "blue", lwd = 2,
     main = "Curva ROC",
     xlab = "False Positive Rate", ylab = "True Positive Rate")

trazar_roc(model_dt,  testData, "red")
trazar_roc(model_nn,  testData, "green")
trazar_roc(model_knn, testData, "orange")
trazar_roc(model_svm, testData, "purple")

legend("bottomright", legend = c("Naive Bayes", "Decision Tree", "Neural Network",
                                 "KNN", "SVM (linear)"),
       col = c("blue", "red", "green", "orange", "purple"), lwd = 2)

```

# Sección 2: Preguntas

**Q1. ¿Si el modelo A tiene mayor Accuracy que B, siempre tendrá mayor Kappa que B? Justifica tu respuesta.**

No hay una correlación directa entre Accuracy y Kappa, el accuracy mide la proporción de aciertos, mientras que kappa compara los aciertos totales contra los valores que podrían darse por el azar, aquí un ejemplo:
```{r}
library(caret)

actual <- factor(c(rep("Positivo", 90), rep("Negativo", 10)),
                 levels = c("Positivo", "Negativo"))

pred_A <- factor(rep("Positivo", 100), levels = c("Positivo", "Negativo"))

pred_B <- c(rep("Positivo", 70), rep("Negativo", 30))
pred_B <- factor(pred_B, levels = c("Positivo", "Negativo"))

cm_A <- confusionMatrix(pred_A, actual)
cm_B <- confusionMatrix(pred_B, actual)

acc_A <- cm_A$overall["Accuracy"]
kappa_A <- cm_A$overall["Kappa"]

acc_B <- cm_B$overall["Accuracy"]
kappa_B <- cm_B$overall["Kappa"]

results_table <- data.frame(
        Model = c("Clasificador A", "Clasificador B"),
        Accuracy = c(as.numeric(acc_A), as.numeric(acc_B)),
        Kappa = c(as.numeric(kappa_A), as.numeric(kappa_B))
)
print(results_table)

```

En este ejemplo, tenemos un dataset desbalanceado en 90 10, el primer clasificador siempre predice positivo, por lo que su accuracy es 0.9, mientras que su kappa, al ser su accuracy esperado igual al accuracy real, es 0, mientras que el clasificador b predice los 70 primeros valores como positivos, y el resto como negativos, por lo que el accuracy esperado es de pe = (0.9x0.7) + (0.1*0.3) = 0.66, al tener el modelo un accuracy superior al esperado, el kappa es positivo, en este caso 0.411, lo que es superior al clasificador A, teniendo un accuracy inferior.

**Q2. ¿Vemos eso en tus resultados?**
*Respuesta:*
No, en mis resultados se puede observar una correlación entre el accuracy y el kappa.

**Q3. ¿Te cambian los resultados cuando cambias la semilla?**

Sí, al cambiar la semilla se generan particiones diferentes de los datos en entrenamiento y test, lo que modifica el rendimiento de los modelos al ser entrenados y probados con un reparto diferente datos.

**Q4. ¿Es recomendable quedarse con los resultados mejores después de cambiar las semillas varias veces? Justifica la respuesta.**

No, no es recomendable seleccionar el resultado mejor obtenido, ya que esto implica un sesgo de selección, lo que puede hacer que nuestro modelo solo sea util para ese set de datos concreto.

**Q5. ¿Qué modelos puedes descartar porque van a ser siempre subóptimos (asumiendo una buena evaluación)?**

Podemos descartar los modelos que solamente predicen una de las clases.

**Q6. ¿Por qué puedes descartar esos modelos?**

Porque, pese a que puedan obtener "buenas métricas", al tener sobre todo datasets desbalanceados, otros modelos pueden aprender realmente de dichos datos y presentar mejores resultados aunque tengan peores métricas